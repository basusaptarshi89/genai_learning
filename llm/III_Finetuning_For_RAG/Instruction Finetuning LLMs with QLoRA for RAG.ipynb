{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Finetuning LLMs with QLoRA for RAG\n",
    "\n",
    "Large Language Models are typically trained as models that simply predict the next word in a sequence.  While this leads to very powerful machines, they don't typically come equipped to deal with certain behaviors, such as following instructions.  In this lab, we will demonstrate how to fine tune a base Large Language Model to better respond to instructions with context, which is a requirement for RAG.  By fine tuning the model in this way, we can teach it to stop better, hallucinate less, and generally behave in a more desirable way.\n",
    "\n",
    "## **Important Note**\n",
    "\n",
    "***We are finetuning a base model for RAG for instructional purposes on how finetuning can change the behavior of models. In practice, many models provide instruction fine-tuned models which will give better results than we can produce here for RAG because they are trained on many more data examples. For example (mistralai/Mistral-7B-v0.1 vs mistralai/Mistral-7B-Instruct-v0.1) and (meta-llama/Llama-2-7b-hf vs. meta-llama/Llama-2-7b-chat-hf). Try and get the best performance out of the finetuning but don't expect it to work perfectly..***\n",
    "\n",
    "- [Preparing the Dataset](#preparing-the-dataset)\n",
    "- [Selecting the Base Pre-trained Model](#selecting-the-base-pre-trained-model)\n",
    "- [Finetuning the Model](#finetuning-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "Fine-tuning LLMs is primarily used for teaching the model new behavior, such as better responding to instructions, responding with certain tones, or acting more as a conversational chatbot.  \n",
    "\n",
    "The dataset for finetuning LLMs are text entries formatted in the way ***THAT WE WISH FOR AN INTERACTION WITH THE MODEL TO LOOK LIKE***.  For example, if we wish for the model to follow instructions better with context, we should provide a dataset which gives examples of it following instructions provided with context.  **This is almost exactly like few-shot prompting, but reinforcing the behavior even further by actually modifying some of the weights of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few tips from ChatGPT:\n",
    "\n",
    "Generative Dataset:\n",
    "\n",
    "    1. Include a dataset of input queries or prompts along with human-generated responses. This is your generative dataset.\n",
    "\n",
    "    2. Make sure that the responses are diverse, well-written, and contextually appropriate for the given queries.\n",
    "\n",
    "    3. It's important to have a variety of responses to encourage the model to generate creative and contextually relevant answers.\n",
    "\n",
    "Training Data Quality:\n",
    "\n",
    "    1. Ensure that your training dataset is of high quality and accurately represents the task you are fine-tuning for.\n",
    "\n",
    "    2. Remove any instances that contain incorrect or misleading information.\n",
    "\n",
    "    3. Filter out instances in your training data where the model is likely to hallucinate or generate incorrect information.\n",
    "\n",
    "    4. Manually review and filter out examples that may lead to misinformation.\n",
    "\n",
    "    5. Use data augmentation techniques to artificially increase the diversity of your dataset. However, be cautious with augmentation to ensure that the generated samples remain contextually relevant and accurate.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset using `datasets`\n",
    "\n",
    "The dataset that we will be using for instruction fine-tuning is a dataset hand-curated by databricks for instruction following called \"dolly-15k\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "      <th>keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td></td>\n",
       "      <td>Camels use the fat in their humps to keep them...</td>\n",
       "      <td>open_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td></td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "      <td>open_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If I have more pieces at the time of stalemate...</td>\n",
       "      <td>Stalemate is a situation in chess where the pl...</td>\n",
       "      <td>No. \\nStalemate is a drawn position. It doesn'...</td>\n",
       "      <td>information_extraction</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>Who is the creator of Python?</td>\n",
       "      <td></td>\n",
       "      <td>Guido van Rossum is the father of Python. And ...</td>\n",
       "      <td>open_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15001</th>\n",
       "      <td>What are common florals found in Zigalga Natio...</td>\n",
       "      <td>Zigalga National Park (Russian: Национальный п...</td>\n",
       "      <td>Zigalga National Park has the majority of its ...</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>What is linux Bootloader</td>\n",
       "      <td>A bootloader, also spelled as boot loader or c...</td>\n",
       "      <td>A bootloader is a program written in machine c...</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15005</th>\n",
       "      <td>What is one-child policy?</td>\n",
       "      <td>The term one-child policy refers to a populati...</td>\n",
       "      <td>The \"one-child policy\" was a Chinese populatio...</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15008</th>\n",
       "      <td>What is the difference between a road bike and...</td>\n",
       "      <td></td>\n",
       "      <td>Road bikes are built to be ridden on asphalt a...</td>\n",
       "      <td>open_qa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7021 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "0             When did Virgin Australia start operating?   \n",
       "2         Why can camels survive for long without water?   \n",
       "3      Alice's parents have three daughters: Amy, Jes...   \n",
       "4                        When was Tomoaki Komorida born?   \n",
       "5      If I have more pieces at the time of stalemate...   \n",
       "...                                                  ...   \n",
       "14999                      Who is the creator of Python?   \n",
       "15001  What are common florals found in Zigalga Natio...   \n",
       "15003                           What is linux Bootloader   \n",
       "15005                          What is one-child policy?   \n",
       "15008  What is the difference between a road bike and...   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Virgin Australia, the trading name of Virgin A...   \n",
       "2                                                          \n",
       "3                                                          \n",
       "4      Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "5      Stalemate is a situation in chess where the pl...   \n",
       "...                                                  ...   \n",
       "14999                                                      \n",
       "15001  Zigalga National Park (Russian: Национальный п...   \n",
       "15003  A bootloader, also spelled as boot loader or c...   \n",
       "15005  The term one-child policy refers to a populati...   \n",
       "15008                                                      \n",
       "\n",
       "                                                response  \\\n",
       "0      Virgin Australia commenced services on 31 Augu...   \n",
       "2      Camels use the fat in their humps to keep them...   \n",
       "3                The name of the third daughter is Alice   \n",
       "4             Tomoaki Komorida was born on July 10,1981.   \n",
       "5      No. \\nStalemate is a drawn position. It doesn'...   \n",
       "...                                                  ...   \n",
       "14999  Guido van Rossum is the father of Python. And ...   \n",
       "15001  Zigalga National Park has the majority of its ...   \n",
       "15003  A bootloader is a program written in machine c...   \n",
       "15005  The \"one-child policy\" was a Chinese populatio...   \n",
       "15008  Road bikes are built to be ridden on asphalt a...   \n",
       "\n",
       "                     category  keep  \n",
       "0                   closed_qa  True  \n",
       "2                     open_qa  True  \n",
       "3                     open_qa  True  \n",
       "4                   closed_qa  True  \n",
       "5      information_extraction  True  \n",
       "...                       ...   ...  \n",
       "14999                 open_qa  True  \n",
       "15001               closed_qa  True  \n",
       "15003               closed_qa  True  \n",
       "15005               closed_qa  True  \n",
       "15008                 open_qa  True  \n",
       "\n",
       "[7021 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'context', 'response'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td></td>\n",
       "      <td>Camels use the fat in their humps to keep them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td></td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If I have more pieces at the time of stalemate...</td>\n",
       "      <td>Stalemate is a situation in chess where the pl...</td>\n",
       "      <td>No. \\nStalemate is a drawn position. It doesn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Based on this paragraph, how many times has Jo...</td>\n",
       "      <td>Born in Scranton, Pennsylvania, Biden moved wi...</td>\n",
       "      <td>Based on this paragraph, Joe Biden has run for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Which Dutch actress played Xenia Onatopp in th...</td>\n",
       "      <td></td>\n",
       "      <td>Dutch actress Marijke Janssen played Xenia Ona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Which of the 10 largest earthquakes was the de...</td>\n",
       "      <td>Largest Earthquakes ever recorded\\nThe 10 larg...</td>\n",
       "      <td>The 2004 earthquake on Sumatra, Indonesia was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Extract the names of the cities and rank them ...</td>\n",
       "      <td>The United Kingdom is a constitutional monarch...</td>\n",
       "      <td>The cities listed in alphabetical order are Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>From the passage provided, extract the date wh...</td>\n",
       "      <td>The 34th Wisconsin Infantry Regiment was a con...</td>\n",
       "      <td>The 34th Wisconsin Infantry Regiment was organ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  \\\n",
       "0           When did Virgin Australia start operating?   \n",
       "1       Why can camels survive for long without water?   \n",
       "2    Alice's parents have three daughters: Amy, Jes...   \n",
       "3                      When was Tomoaki Komorida born?   \n",
       "4    If I have more pieces at the time of stalemate...   \n",
       "..                                                 ...   \n",
       "195  Based on this paragraph, how many times has Jo...   \n",
       "196  Which Dutch actress played Xenia Onatopp in th...   \n",
       "197  Which of the 10 largest earthquakes was the de...   \n",
       "198  Extract the names of the cities and rank them ...   \n",
       "199  From the passage provided, extract the date wh...   \n",
       "\n",
       "                                               context  \\\n",
       "0    Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "4    Stalemate is a situation in chess where the pl...   \n",
       "..                                                 ...   \n",
       "195  Born in Scranton, Pennsylvania, Biden moved wi...   \n",
       "196                                                      \n",
       "197  Largest Earthquakes ever recorded\\nThe 10 larg...   \n",
       "198  The United Kingdom is a constitutional monarch...   \n",
       "199  The 34th Wisconsin Infantry Regiment was a con...   \n",
       "\n",
       "                                              response  \n",
       "0    Virgin Australia commenced services on 31 Augu...  \n",
       "1    Camels use the fat in their humps to keep them...  \n",
       "2              The name of the third daughter is Alice  \n",
       "3           Tomoaki Komorida was born on July 10,1981.  \n",
       "4    No. \\nStalemate is a drawn position. It doesn'...  \n",
       "..                                                 ...  \n",
       "195  Based on this paragraph, Joe Biden has run for...  \n",
       "196  Dutch actress Marijke Janssen played Xenia Ona...  \n",
       "197  The 2004 earthquake on Sumatra, Indonesia was ...  \n",
       "198  The cities listed in alphabetical order are Be...  \n",
       "199  The 34th Wisconsin Infantry Regiment was organ...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def load_modified_dataset():\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split = \"train\")\n",
    "    df = dataset.to_pandas()\n",
    "    df['keep'] = True\n",
    "    \n",
    "    # Keep entries with correct answer as well\n",
    "    df = df[(df['category'].isin(['closed_qa', 'information_extraction', 'open_qa'])) & df['keep']]\n",
    "    display(df)\n",
    "    \n",
    "    return Dataset.from_pandas(\n",
    "        df[['instruction', 'context', 'response']], \n",
    "        preserve_index = False)\n",
    "    \n",
    "dataset = load_modified_dataset()\n",
    "dataset = dataset.select(range(200))\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "display(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base dataset contains columns for an `instruction`, an optional `context`, and a `response` that we want the bot to respond to.  However, to feed it into the model for finetuning, we need to combine each column so that 1 sample corresponds to 1 example interaction with the model.  \n",
    "\n",
    "This 1 sample should be an example to the LLM about:\n",
    "\n",
    "1. How we wish to interact with the model (prompt)\n",
    "2. How we want the model to respond\n",
    "\n",
    "Remember, these generative LLMs are trained to read in a provided prompt, and essentially auto-complete the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def format_instruction(sample : Dict) -> str:\n",
    "    \"\"\"Combine a row to a single str\"\"\"\n",
    "    return f\"\"\"### Context:\n",
    "{sample['context']}\n",
    "\n",
    "### Question:\n",
    "Using only the context above, {sample['instruction']}\n",
    "\n",
    "### Response:\n",
    "{sample['response']}\n",
    "\n",
    "This advice was from basu.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide this as the entire prompt to the model for training, using the Causal Language Modeling objective for loss.\n",
    "\n",
    "```\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "Using only the context above, {instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Base Pre-trained Model\n",
    "\n",
    "Once we have the data, we can select the base model that we would like to fine tune for this behavior.  \n",
    "\n",
    "The model that we will select is the `mistralai/mistral-7b` base model.  This is a 7.3b parameter model, quite small in the grand scheme of LLMs, but one that produces good quality results, especially compared to many other open source models.\n",
    "\n",
    "### Quantization using `bitsandbytes`\n",
    "\n",
    "LLMs are extremely memory intensive.  One trick that is commonly used when working with LLMs to reduce memory usage as well as increase computational speed for both inference and training, is reducing the precision of the weights from full precision 32-bit floating points (fp32) to lower precisions such as int8, fp4, nf4, etc.  This is known as quantization.  Research has shown that quantization often times has minimal impact on the quality of generations, but this is on a case-by-case basis. \n",
    "\n",
    "In this example, we will be quantizing and fine-tuning using normal-float 4 bit (nf4).  In practice, the quantization behind the scenes is handled by the `bitsandbytes` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005080699920654297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 48,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35d2f80007a42a88f2d6839b892d877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face Base Model ID\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "is_peft = False\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "if is_peft:\n",
    "    # load base LLM model with PEFT Adapter\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_flash_attention_2=True,\n",
    "        quantization_config = bnb_config\n",
    "    )\n",
    "else:\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config = bnb_config,\n",
    "        use_flash_attention_2=True\n",
    "    )\n",
    "\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model loaded up, we are ready to finetune using our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the Model\n",
    "\n",
    "There are two main ways to finetune a large language model:\n",
    "\n",
    "1. Pre-training/Full Finetuning\n",
    "\n",
    "    In this situation, all of the model weights (all 7b of them) are set to be trainable and tweaked during training.  This can lead to the most dramatic changes in model behavior but is also the most computationally expensive.  \n",
    "    \n",
    "    When initially training the model, also known as pre-training, this is necessarily done and where you see the extreme computational costs show up (i.e. 500 A100 80GB GPUs trained for 10000 hours, etc...).\n",
    "\n",
    "2. Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "    Parameter efficient finetuning methods are an alternative to full finetuning where, instead of training the parameters of the pre-trained model, a subset of new parameters are trained without touching the base model weights. These new trainable parameters are injected into the model mathematically at different points to change the outcome.  There are a handful of methods that use this approach such as Prompt Tuning, P-Tuning, and Low-Rank Adaptation (LoRA).  For this lab, we will focus on LoRA.  \n",
    "\n",
    "    LoRA methods introduce a set of trainable rank-decomposition matrices (update matrices) which can be used to modify the existing weights of the pre-trained model.  The typical location that these matrices are placed are within the attention layers, so they are not exclusive to LLMs.  The size of these update matrices can be controlled by  setting the desired rank of the matrix (lora_r), with smaller rank corresponding to smaller matrices and thus fewer trainable parameters.   During fine-tuning, only these update matrices are tuned and often times, this makes the total number of trainable parameters a very small fraction of the total number of weights.\n",
    "\n",
    "### Finetuning using `peft`\n",
    "\n",
    "To configure the model for paremeter efficient fine-tuning and LoRA, we will use the `peft` package.  Specifically, we will define our Lora parameters and also set to the taks to `CAUSAL_LM` to train the model for generative purposes.  Because we also quantized the model to 4-bit, we will also be using a state-of-the-art method called Quantized LoRA (QLoRA) to do this training in low precision to save memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "if is_peft:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model._mark_only_adapters_as_trainable()\n",
    "else:\n",
    "    # LoRA config for QLoRA\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['v_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj', 'gate_proj', 'k_proj']\n",
    "    )\n",
    "\n",
    "    # prepare model for training with low-precision\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the trainer with `trl`\n",
    "\n",
    "Now that we have prepared the data, loaded the model in 4-bit, and configured our LoRA finetuning according to our model, we are ready to train the model. Training of LLMs for generative purposes uses the causal language modeling objective.  Briefly, this specifies that when calculating attention, the model should only be able to consider things \"to the left\".  So for a sentence, it can only decide what to generate by looking at all of the words that came before it.  \n",
    "\n",
    "A very useful wrapper for training transformer based models is the Supervised Fine-Tuning Trainer (`SFTrainer`) provided by the `trl` library.  While the supervised fine tuning is typically used in the context of reinforcement learning, for our purposes, it simply refers to providing the model with examples of input, and response.  All of the actual training, including computing gradients, tweaking the optimizer, batching the data, evaluation will be done behind the scenes using the `SFTrainer` wrapper.  This will conduct the finetuning that we want after we pass in the dataset and hyperparameters.  This is much more efficient and robust than writing our own training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003924131393432617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 48,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85f0924e0214bcebb73278d06b38af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./mistral-7b-int4-dolly\", \n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=5, # batch size per batch\n",
    "    gradient_accumulation_steps=2, # effective batch size\n",
    "    gradient_checkpointing=True, \n",
    "    gradient_checkpointing_kwargs={'use_reentrant':True},\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=1, # log the training error every 10 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit = 2, # save 2 total checkpoints\n",
    "    ignore_data_skip=True,\n",
    "    save_steps=2, # save a checkpoint every 1 steps\n",
    "    learning_rate=1e-3,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=5,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True\n",
    ")\n",
    "\n",
    "# https://huggingface.co/docs/trl/sft_trainer#packing-dataset--constantlengthdataset-\n",
    "# max seq length for packing\n",
    "max_seq_length = 2048 \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction, # our formatting function which takes a dataset row and maps it to str\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of the configuration done, we can now run our training.  On an A10g, this takes about 1 hours to run, after which it will save the LoRA weights to the `mistral-7b-int4-dolly` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4661, 'learning_rate': 0.001, 'epoch': 0.01}\n",
      "{'loss': 1.4757, 'learning_rate': 0.001, 'epoch': 0.02}\n",
      "{'loss': 1.441, 'learning_rate': 0.001, 'epoch': 0.03}\n",
      "{'loss': 1.4502, 'learning_rate': 0.001, 'epoch': 0.05}\n",
      "{'loss': 1.5181, 'learning_rate': 0.001, 'epoch': 0.06}\n",
      "{'loss': 1.4361, 'learning_rate': 0.001, 'epoch': 0.07}\n",
      "{'loss': 1.3195, 'learning_rate': 0.001, 'epoch': 0.08}\n",
      "{'loss': 1.366, 'learning_rate': 0.001, 'epoch': 0.09}\n",
      "{'loss': 1.5214, 'learning_rate': 0.001, 'epoch': 0.1}\n",
      "{'loss': 1.3335, 'learning_rate': 0.001, 'epoch': 0.12}\n",
      "{'loss': 1.3567, 'learning_rate': 0.001, 'epoch': 0.13}\n",
      "{'loss': 1.1749, 'learning_rate': 0.001, 'epoch': 0.14}\n",
      "{'loss': 1.3328, 'learning_rate': 0.001, 'epoch': 0.15}\n",
      "{'loss': 1.3411, 'learning_rate': 0.001, 'epoch': 0.16}\n",
      "{'loss': 1.2879, 'learning_rate': 0.001, 'epoch': 0.17}\n",
      "{'loss': 1.2433, 'learning_rate': 0.001, 'epoch': 0.18}\n",
      "{'loss': 1.3204, 'learning_rate': 0.001, 'epoch': 0.2}\n",
      "{'loss': 1.2923, 'learning_rate': 0.001, 'epoch': 0.21}\n",
      "{'loss': 1.2896, 'learning_rate': 0.001, 'epoch': 0.22}\n",
      "{'loss': 1.0911, 'learning_rate': 0.001, 'epoch': 0.23}\n",
      "{'loss': 1.2978, 'learning_rate': 0.001, 'epoch': 0.24}\n",
      "{'loss': 1.1709, 'learning_rate': 0.001, 'epoch': 0.25}\n",
      "{'loss': 1.2764, 'learning_rate': 0.001, 'epoch': 0.27}\n",
      "{'loss': 1.3446, 'learning_rate': 0.001, 'epoch': 0.28}\n",
      "{'loss': 1.7988, 'learning_rate': 0.001, 'epoch': 0.29}\n",
      "{'loss': 1.2974, 'learning_rate': 0.001, 'epoch': 0.3}\n",
      "{'loss': 1.225, 'learning_rate': 0.001, 'epoch': 0.31}\n",
      "{'loss': 1.2099, 'learning_rate': 0.001, 'epoch': 0.32}\n",
      "{'loss': 1.3837, 'learning_rate': 0.001, 'epoch': 0.34}\n",
      "{'loss': 1.2597, 'learning_rate': 0.001, 'epoch': 0.35}\n",
      "{'loss': 1.1578, 'learning_rate': 0.001, 'epoch': 0.36}\n",
      "{'loss': 1.0226, 'learning_rate': 0.001, 'epoch': 0.37}\n",
      "{'loss': 1.2218, 'learning_rate': 0.001, 'epoch': 0.38}\n",
      "{'loss': 1.3999, 'learning_rate': 0.001, 'epoch': 0.39}\n",
      "{'loss': 1.1915, 'learning_rate': 0.001, 'epoch': 0.4}\n",
      "{'loss': 1.2782, 'learning_rate': 0.001, 'epoch': 0.42}\n",
      "{'loss': 1.1808, 'learning_rate': 0.001, 'epoch': 0.43}\n",
      "{'loss': 1.236, 'learning_rate': 0.001, 'epoch': 0.44}\n",
      "{'loss': 1.3014, 'learning_rate': 0.001, 'epoch': 0.45}\n",
      "{'loss': 1.2109, 'learning_rate': 0.001, 'epoch': 0.46}\n",
      "{'loss': 1.1214, 'learning_rate': 0.001, 'epoch': 0.47}\n",
      "{'loss': 1.2344, 'learning_rate': 0.001, 'epoch': 0.49}\n",
      "{'loss': 1.224, 'learning_rate': 0.001, 'epoch': 0.5}\n",
      "{'loss': 1.2174, 'learning_rate': 0.001, 'epoch': 0.51}\n",
      "{'loss': 1.2186, 'learning_rate': 0.001, 'epoch': 0.52}\n",
      "{'loss': 1.3035, 'learning_rate': 0.001, 'epoch': 0.53}\n",
      "{'loss': 1.2104, 'learning_rate': 0.001, 'epoch': 0.54}\n",
      "{'loss': 1.2355, 'learning_rate': 0.001, 'epoch': 0.55}\n",
      "{'loss': 1.2196, 'learning_rate': 0.001, 'epoch': 0.57}\n",
      "{'loss': 1.2284, 'learning_rate': 0.001, 'epoch': 0.58}\n",
      "{'loss': 1.2777, 'learning_rate': 0.001, 'epoch': 0.59}\n",
      "{'loss': 1.371, 'learning_rate': 0.001, 'epoch': 0.6}\n",
      "{'loss': 1.1619, 'learning_rate': 0.001, 'epoch': 0.61}\n",
      "{'loss': 1.2345, 'learning_rate': 0.001, 'epoch': 0.62}\n",
      "{'loss': 1.4296, 'learning_rate': 0.001, 'epoch': 0.64}\n",
      "{'loss': 1.1625, 'learning_rate': 0.001, 'epoch': 0.65}\n",
      "{'loss': 1.3557, 'learning_rate': 0.001, 'epoch': 0.66}\n",
      "{'loss': 1.1417, 'learning_rate': 0.001, 'epoch': 0.67}\n",
      "{'loss': 1.356, 'learning_rate': 0.001, 'epoch': 0.68}\n",
      "{'loss': 1.1546, 'learning_rate': 0.001, 'epoch': 0.69}\n",
      "{'loss': 1.1562, 'learning_rate': 0.001, 'epoch': 0.71}\n",
      "{'loss': 1.3042, 'learning_rate': 0.001, 'epoch': 0.72}\n",
      "{'loss': 1.1896, 'learning_rate': 0.001, 'epoch': 0.73}\n",
      "{'loss': 1.3811, 'learning_rate': 0.001, 'epoch': 0.74}\n",
      "{'loss': 1.3347, 'learning_rate': 0.001, 'epoch': 0.75}\n",
      "{'loss': 1.3215, 'learning_rate': 0.001, 'epoch': 0.76}\n",
      "{'loss': 1.1404, 'learning_rate': 0.001, 'epoch': 0.77}\n",
      "{'loss': 1.2373, 'learning_rate': 0.001, 'epoch': 0.79}\n",
      "{'loss': 1.2794, 'learning_rate': 0.001, 'epoch': 0.8}\n",
      "{'loss': 1.33, 'learning_rate': 0.001, 'epoch': 0.81}\n",
      "{'loss': 1.3149, 'learning_rate': 0.001, 'epoch': 0.82}\n",
      "{'loss': 1.1826, 'learning_rate': 0.001, 'epoch': 0.83}\n",
      "{'loss': 1.2914, 'learning_rate': 0.001, 'epoch': 0.84}\n",
      "{'loss': 1.2384, 'learning_rate': 0.001, 'epoch': 0.86}\n",
      "{'loss': 0.8889, 'learning_rate': 0.001, 'epoch': 0.87}\n",
      "{'loss': 1.3544, 'learning_rate': 0.001, 'epoch': 0.88}\n",
      "{'loss': 1.3342, 'learning_rate': 0.001, 'epoch': 0.89}\n",
      "{'loss': 1.236, 'learning_rate': 0.001, 'epoch': 0.9}\n",
      "{'loss': 1.205, 'learning_rate': 0.001, 'epoch': 0.91}\n",
      "{'loss': 1.3811, 'learning_rate': 0.001, 'epoch': 0.92}\n",
      "{'loss': 1.1579, 'learning_rate': 0.001, 'epoch': 0.94}\n",
      "{'loss': 1.3662, 'learning_rate': 0.001, 'epoch': 0.95}\n",
      "{'loss': 1.2631, 'learning_rate': 0.001, 'epoch': 0.96}\n",
      "{'loss': 1.2748, 'learning_rate': 0.001, 'epoch': 0.97}\n",
      "{'loss': 1.6631, 'learning_rate': 0.001, 'epoch': 0.98}\n",
      "{'loss': 1.2792, 'learning_rate': 0.001, 'epoch': 0.99}\n",
      "{'train_runtime': 2525.711, 'train_samples_per_second': 0.341, 'train_steps_per_second': 0.034, 'train_loss': 1.2847299707490345, 'epoch': 0.99}\n",
      "2526.2656712532043s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainer.train(resume_from_checkpoint=False) # progress bar is fake due to packing\n",
    "trainer.save_model()\n",
    "end = time.time()\n",
    "print(f\"{end - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has finished training, it is ready to be used.  Now, hopefully, when the model sees the prompt that we crafted before, it will know how to respond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
